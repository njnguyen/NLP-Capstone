# datasets
train_dataset_path: data/hindi/hi_oscar_cleaned_train.txt
sharded_train_dataset: False
val_dataset_path: data/hindi/hi_oscar_cleaned_dev.txt
mlm_masking_prob: 0.15
max_seq_len: 256

# training
seed: 1
hf_model: xlm-roberta-base
learning_rate: 1e-5
training_steps: 500
train_batch_size: 5
gradient_accumulation_steps: 2
logging_steps: 250
eval_strategy: steps
eval_steps: 150
eval_batch_size: 10
checkpoints_directory: models/hindi/cpt-full
save_steps: 150
saved_checkpoints_limit: 2
early_stopping_patience: 10
load_best_model_at_end: True

# advanced training options
new_vocab_file: null
new_embedding_path: null
freeze_main_model: False
unfreeze_step_ratio: null
model_freeze_prefix: null
weight_decay: 0.0
lr_scheduler_type: "linear"
warmup_ratio: 0.0
warmup_steps: 0.0 # overrides warmup ratio
auto_find_batch_size: False # requires accelerate library
group_by_length: False # reduce padding effect
gradient_checkpointing: False
fp16: False # mixed precision training
torch_distributed_training: False # fsdp parameter in HF code
full_determinism: False # ensure reproducible results in distributed training